ggplot(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_point(size = 0.5) +
facet_grid(continent~year, scales = "free")
ggplot()
ggplot() +
geom_point(mapping = aes(x = 799, y = 28.8))
ggplot() +
geom_point(mapping = aes(x = 799, y = 28.8)) +
geom_point(mapping = aes(x = 821, y = 30.3))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = depPercap, y = lifeExp))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_smooth(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp))
ggplot(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_point() +
geom_smooth() +
xlab("New name for x-axis") +
ylab("New name for y-axis") +
theme_light()
setwd("~/GitHub/CompLegFall2019/data/uk_lower/Calendars")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "lubridate"), pkgTest)
# construct url
base = "https://hansard.parliament.uk/Commons/1990-11-09"
# construct url
url = "https://hansard.parliament.uk/Commons/1990-11-09"
cont <- url %>%
read_html() %>%
html_nodes("#sectionTree > .no-children a") %>%
html_text()
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
cont
# construct url
url = "https://hansard.parliament.uk/Commons/1990-11-10"
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
cont
# construct url
yr = 1999
mo = 11
day = 07
date <- paste0(mo,"/",day,"/",yr)
day = "07"
date <- paste0(mo,"/",day,"/",yr)
url = "https://hansard.parliament.uk/Commons/1990-11-07"
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
}
dates <- character(0)
dates <- c(dates,date)
# Create empty vector
dates <- character(0)
# Loop thru dates
for (yr in 1990:1991) {
for (mo in 1:12) {
for (day in 1:31) {
if(day<10) {
day = paste0("0",day)
}
if(mo<10) {
mo = paste0("0",mo)
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
}
}
}
View(dates)
mo = 9
day = 7
if(day<10) {
day = paste0("0",day)
}
if(mo<10) {
mo = paste0("0",mo)
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
if(day<10) {
day = paste0("0",day)
}
mo = 9
day = 7
if(day<10) {
day = paste0("0",day)
}
if(mo<10) {
mo = paste0("0",mo)
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
url
# construct url
yr = 1990
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
# Create empty vector
dates <- character(0)
# Loop thru dates
for (yr in 1990:1990) {
for (mo in 1:9) {
for (day in 1:31) {
if(day<10) {
day = paste0("0",day)
}
if(mo<10) {
mo = paste0("0",mo)
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
}
}
}
testd <- c(testd,paste0(mo,"/",day,"/",yr))
testd<- character(0)
testd <- c(testd,paste0(mo,"/",day,"/",yr))
# Create empty vector
dates <- character(0)
testd<- character(0)
# Loop thru dates
for (yr in 1990:1990) {
for (mo in 1:3) {
for (day in 1:31) {
if(day<10) {
day = paste0("0",day)
}
if(mo<10) {
mo = paste0("0",mo)
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
testd <- c(testd,paste0(mo,"/",day,"/",yr))
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
}
}
}
# Create empty vector
dates <- character(0)
testd<- character(0)
# Loop thru dates
for (yr in 1990:1990) {
for (mon in 1:3) {
for (da in 1:31) {
if(da<10) {
day = paste0("0",da)
}
if(da>=10) {
day = da
}
if(mon<10) {
mo = paste0("0",mon)
}
if(mon>=10) {
mo = mon
}
base = "https://hansard.parliament.uk/Commons/"
url = paste0(base,yr,"-",mo,"-",day)
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
testd <- c(testd,paste0(mo,"/",day,"/",yr))
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
}
}
}
View(dates)
# Create empty vector
dates <- character(0)
# Loop thru dates
for (yr in 1990:2019) {
for (mon in 1:12) {
for (da in 1:31) {
# add 0 to single digit month/day
if(da<10) {
day = paste0("0",da)
}
if(da>=10) {
day = da
}
if(mon<10) {
mo = paste0("0",mon)
}
if(mon>=10) {
mo = mon
}
# base url
base = "https://hansard.parliament.uk/Commons/"
# construct url
url = paste0(base,yr,"-",mo,"-",day)
# check if there is a meeting
cont <- url %>%
read_html() %>%
html_node("#sectionTree > .no-children a") %>%
html_text()
# add to vector
if(!is.na(cont)) {
date <- paste0(mo,"/",day,"/",yr)
dates <- c(dates,date)
}
}
}
}
write.csv(dates,"uk_lower_calendar.csv")
setwd("C:/Users/jenni/Desktop/Jennifer/QTM 150")
library(tidyverse)
diabobes <- read.csv("obesity-diabetes.csv")
typeof(c(T,T,F,T))
typeof(c(T,T,F,NA,NA,T))
typeof(c(1,2,3))
c("asdf","c","asara  af")
c(5)
typeof(5)
typeof(c(5))
`+`(13,45)
"awer">"aeasdf"
c(1,"2",4)
c(T, 1, 2, NA, F)
c(1,2,4,1,3,4) > c(1,2)
# actually
c(1,2,4,1,3,4) > c(1,2,1,2,1,2)
c(1,2,4,1,3,4) > c(1,2,3,4)
# actually
c(1,2,4,1,3,4) > c(1,2,3,4,1,2)
new_diab <- diabobes %>%
mutate(diab = as.character(diab))
x <- c(4,8,5,6,3)
x[3]
x[3:5]
x[c(3,5)]
setwd("~/GitHub/CompLegFall2019/data/canadaTextParsing/Examples")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
# create function to clean speeches
# to create DTM
###################################
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
speechesDF[50,"paragraph_text"]
##################################
# Doing this all using library(tm)
##################################
# create new corpus object with tm
speech_corpus <- Corpus(VectorSource(speechesDF$paragraph_text))
# clean corpus as we did before
speech_corpus <- tm_map(speech_corpus, content_transformer(tolower))
speech_corpus <- tm_map(speech_corpus, removeNumbers)
speech_corpus <- tm_map(speech_corpus, removePunctuation)
speech_corpus <- tm_map(speech_corpus, removeWords, c("the", "and", stopwords("english")))
speech_corpus <- tm_map(speech_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
# check to see what it looks like
inspect(alternative_DTM[1:50, 1:50])
str(alternative_DTM)
View(alternative_DTM)
alternative_DTM[[1]]
alternative_DTM[[6]]
terms <- alternative_DTM[[6]]
str[terms]
str(terms)
terms[[1]]
temrs[[2]]
terms[[2]]
terms <- alternative_DTM[[6]][[2]]
alternative_DTM[[2]]
freq <- alternative_DTM[[2]]
str(speech_corpus)
str(alternative_DTM[[1:5]])
View(speech_corpus)
colSums(alternative_DTM)
# create DTM
alternative_DTM <- DocumentTermMatrix(speech_corpus)
View(DTM)
str(DTM)
freq <- colSums(DTM)
str(freq)
View(freq)
IDF = log(94/freq)
View(IDF)
sum(freq==0)
names(freq)
speech_corpus[[1]]
IDF["tower"]
i = names(freq)[1]
i
s = speech_corpus[1]
s = speech_corpus[[1]]
s = speech_corpus[[1]][1]
s = speech_corpus[[1]][1][1]
s
str_detect(s,i)
for (i in names(freq)) {
count = 0
for (s in speech_corpus) {
if(str_detect(s,i)) {
count=count+1
}
}
IDF[i]=log(94/count)
}
for (i in names(freq)) {
count = 0
for (s in speech_corpus) {
if(str_detect(s[[1]],i)) {
count=count+1
}
}
IDF[i]=log(94/count)
}
for (s in speech_corpus) {
if(str_detect(s[[1]],i)) {
count=count+1
}
}
speech_corpus[2]
s=speech_corpus[2]
s[[1]]
str_detect(s,i)
str(DTM)
str(speechesDF)
DTM[1]
DTM[1:]
DTM[1,]
DTM[2,]
i
DTM[i]
DTM[,i]
sub = subset(DTM, DTM[,i]!=0)
str(sum)
str(sub)
View(DTM[,i]!=0)
sum(DTM[,i]!=0)
count = sum(DTM[,i]!=0)
count
for (i in names(freq)) {
count = sum(DTM[,i]!=0)
IDF[i]=log(94/count)
}
View(IDF)
dis <- freq %*% IDF
dis <- freq * IDF
View(freq)
IDF
dis <- freq %*% IDF
freq
dis <- freq * IDF
View(dis)
idf <- diag(dis)
View(idf)
idf <- diag(IDF)
dis <- freq %*% idf
dis <- freq %*% idf
tf <- as.matrix(t(DTM))
idf_j <- log(ncol(tf)/(rowSums(tf)))
setwd("~/GitHub/CompLegFall2019/labs/DTMs")
tf <- as.matrix(t(DTM))
IDF_j <- log(ncol(tf)/(rowSums(tf)))
idf_j <- diag(IDF_j)
View(idf_j)
