plot(c(1,34,56,23,11))
72+59
72+59
1+3+5
sum(1,5,9)
sum(1,5,9)
1+3+5
sum(1,5,9)
plot(c(1,2,3,4))
library("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
qplot(x = c(1,2,3,4), y=c(2,1,4,3))
library("tidyverse")
qplot(x = c(1,2,3,4), y=c(2,1,4,3))
install.packages("gapminder")
knitr::opts_chunk$set(echo = TRUE)
library(gapminder)
library(tidyverse)
View(gapminder)
qplot(x = gdpPercap, y = lifeExp, data = gapminder)
diabobes <- read.csv("obesity-diabetes.csv")
diabobes
qplot(x = diab,
y = obes,
data = diabobes,
color = region,
shape = region,
size = I(3))
library(tidyverse)
library(gapminder)
qplot(x = diab,
y = obes,
data = diabobes,
color = region,
shape = region,
size = I(3))
View(diabobes)
?<NAME OF THE FUNCTION>
#?<NAME OF THE FUNCTION>
?qplot
qplot(gdpPercap, lifeExp, data = gapminder, facets = ~year)
qplot(x = gdpPercap,
y = lifeExp,
geom = c('point', 'smooth'),
data = gapminder)
qplot(x = gdpPercap,
y = lifeExp,
geom = c('point', 'smooth'),
data = gapminder,
facets = ~year)
library(tidyverse)
diamonds
diamonds <- diamonds
View(diamonds)
qplot(depth, price, color = cut, data = diamonds)
qplot(depth, price, color = I('blue'), data = diamonds)
qplot(depth, price, color = cut, data = diamonds)
library(diamonds)
qplot(depth, price, color = cut, data = diamonds)
install.packages("nycflights13")
library(nycflights13)
flights
filter(flights, month ==3)
# all month, second half of each of them
filter(flights, day >= 15)
#Third day of March
filter(flights, month == 3 & day == 3)
library(tidyverse)
midwest
qplot(popdensity,
percbelowpoverty,
data = midwest,
geom = "point")
qplot(popdensity,
percbelowpoverty,
data = midwest)
qplot(popdensity,
percbelowpoverty,
data = midwest,
facets = _inmetro)
qplot(popdensity,
percbelowpoverty,
data = midwest,
facets = ~inmetro)
View(midwest)
ob <- read.csv("obes_diab_trend.csv")
View(ob)
qplot(year, prevalence, data = ob, facets = ~condition)
qplot(year, prevalence, data = ob, color = ~condition)
qplot(year, prevalence, data = ob, color = condition)
qplot(year, prevalence, data = ob, geom = "line", color = condition)
```{r message=FALSE warning=FALSE}
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidyverse)
library(nycflights13)
library(gapminder)
filter(flights, month==1 | month==2 | month==3 | month==4 | month==5)
View(gapminder)
new_gap <- filter(gapminder, country %in% c("Brazil", "India", "China"))
View(new_gap)
qplot(year, lifeExp, data = new_gap, geom = "line")
qplot(year, lifeExp, data = new_gap, geom = "line", color = country)
mean(c(1,2,3,NA,4,5), na.rm = TRUE)
starwars
View(starwars)
# starwars is a dataset known to have missing values
filter(starwars, is.na(hair_color))
# all characters with hair_color NA or "none"
filter(starwars, hair_color == "none" | is.na(hair_color))
View(flights)
# second parameter is the name of the new column
mutate(flights, in_flight_gain = dep_delay - arr_delay)
mutate(flights, air_time_hours = air_time/60)
mutate(flights, dist_prop_mean = distance/mean(distance))
# gapminder draw gdpPercap v. lifeExp for year 1952
gap52 <- filter(gapminder, year == 1952)
qplot(gdpPercap, lifeExp, data = gap52)
# gapminder draw gdpPercap v. lifeExp for year 1952
gap52 <- filter(gapminder, year == 1952) %>%
mutate(log_gdp = log(gdpPercap))
qplot(log_gdp, lifeExp, data = gap52)
1 %>% sum(2,3)
gapnew <- gapminder %>%
filter(year == 1952) %>%
mutate(log_gdp = log(gdpPercap)) %>%
filter(continent = "Asia") %>%
mutate(tot_gdp = gdpPercap * pop)
gapnew <- gapminder %>%
filter(year == 1952) %>%
mutate(log_gdp = log(gdpPercap)) %>%
filter(continent == "Asia") %>%
mutate(tot_gdp = gdpPercap * pop)
library(gapminder)
gap <- gapminder %>%
filter(year == 2002 | year == 2007)
library(tidyverse)
gap <- gapminder %>%
filter(year == 2002 | year == 2007)
View(gap)
qplot(gdpPercap,
lifeExp,
data = gapminder)
qplot(gdpPercap,
lifeExp,
data = gap)
diabobes <- read.csv("obesity-diabetes.csv")
S_diab <- read.csv("S-obesity-diabetes.csv")
S_inc <- read.csv("ACS-south-2017.csv")
diabobes <- read.csv("obesity-diabetes.csv")
S_diab <- read.csv("S-obesity-diabetes.csv")
S_inc <- read.csv("ACS-south-2017.csv")
qplot(x = gdpPercap,
y = lifeExp,
data = gapminder,
facets = continent~year)
qplot(x = gdpPercap,
y = lifeExp,
data = gapminder,
size = I(0.5),
facets = continent~year)
ggplot(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_point(size = 0.5) +
facet_grid(continent~year)
ggplot(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_point(size = 0.5) +
facet_grid(continent~year, scales = "free")
ggplot()
ggplot() +
geom_point(mapping = aes(x = 799, y = 28.8))
ggplot() +
geom_point(mapping = aes(x = 799, y = 28.8)) +
geom_point(mapping = aes(x = 821, y = 30.3))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = depPercap, y = lifeExp))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp))
ggplot()+
geom_point(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_smooth(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp))
ggplot(data = gapminder,
mapping = aes(x = gdpPercap, y = lifeExp)) +
geom_point() +
geom_smooth() +
xlab("New name for x-axis") +
ylab("New name for y-axis") +
theme_light()
setwd("C:/Users/jenni/Desktop/Jennifer/QTM 150")
library(tidyverse)
library(gapminder)
library(nycflights13)
diabobes <- read.csv("obesity-diabetes.csv")
diabobes[[4]]
diabobes$obes
attributes(diabobes)
View(diabobes)
filter(diabobes, state == "Arizona")
diabobes %>%
filter(state=="Arizona")
diabobes <- read.csv("obesity-diabetes.csv")
diabobes %>%
filter(state=="Arizona")
filter(diabobes, state == "Arizona")
View(diabobes)
diabobes <- read_csv("obesity-diabetes.csv")
filter(diabobes, state == "Arizona")
diabobes[diabobes$state == "Arizona",]
View(flights)
flights %>%
mutate(gain = arr_delay - dep_delay)
flights %>%
mutate(gain = arr_delay - dep_delay)
flights$gain <- flights$arr_delay - flights$dep_time
mutate(flights, avg = rowMeans(cbind(flights$arr_delay, flights$dep_delay)))
c(1,2,3,4,5) > c(1,2)
x <- list(1:5, c(NA, pi,Inf), c("a string","something else"), list(-1, -5))
x[[3]][2]
View(gapminder)
gapminder$pop
View(midwest)
midwest[4:8,2:4]
c(1, 2, "a", NA, TRUE)
str(c(1, 2, "a", NA, TRUE))
x = 1
if(
# logical condition
x>2
){
# what happens if condition is T
print(x)
}
x = 3
if(
# logical condition
x>2
){
# what happens if condition is T
print(x)
}
for (
# Define range of iteration
# define an indexing variable
i in 1:15) {
# what happends every round of iteration
# can depend on indexing variable
print(i)
}
for (i in sentences[10:14]) {
print(i)
}
# name of function
addd <- function(
# parameters of function
x,y
){
# what the function does
z <- x+y
# what it returns as output
z
}
addd(3,8)
spur <- function(y){
new_gap <- filter(gapminder, year==y)
qplot(gdpPercap, lifeExp, data = new_gap)
}
spur(1952)
spur <- function(y){
if(!y %in% unique(gapminder$year)) {
print("year not in data")
} else{
new_gap <- filter(gapminder, year==y)
qplot(gdpPercap, lifeExp, data = new_gap)
}
}
spur(2000)
spur(2002)
gap_1952 <- filter(gapminder, year == 1952)
setwd("~/GitHub/CompLegFall2019/data/uk_lower")
setwd("~/GitHub/CompLegFall2019/data/canadaTextParsing/Examples")
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Jeff wd
setwd('~/Documents/GitHub/CompLegFall2019/data/canadaTextParsing/Examples/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
setwd("~/GitHub/CompLegFall2019/data/canadaTextParsing/Examples")
# load speeches data from 38th parliament
speechesDF <- read.csv("canada_floor_speeches.csv", stringsAsFactors = F, encoding = "UTF-8")
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
setwd("~/GitHub/CompLegFall2019/data")
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "paragraph_text"]))
}
# find unique words in word matrix
unique(all_words$word)
DTM <- matrix(0, nrow=dim(speechesDF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(speechesDF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# (1) squared euclidean distance function
euclidean_distance <- function(points1, points2) {
# create empty distance matrix based on size of two vectors
distanceMatrix <- matrix(NA, nrow = dim(points1)[1], ncol = dim(points2)[1])
# for each row
for(i in 1:nrow(points2)) {
# get the distance
distanceMatrix[, i] <- sqrt(rowSums(t(t(points1) - points2[i, ])^2))
}
distanceMatrix
}
# (2) define k-means algorithm
K_means <- function(x, k, n_iter) {
# create empty vectorized lists based on the size of iterations
clusterHistory <- vector(n_iter, mode="list")
centerHistory <- vector(n_iter, mode="list")
# generate starting centers
# sample some centers, 5 for example
centers <- x[sample(nrow(x), k),]
# iterate for set number of times
for(i in 1:n_iter) {
# calculate distance of each word from center
distances_to_centers <- euclidean_distance(x, centers)
# assign to closest cluster (min)
clusters <- apply(distances_to_centers, 1, which.min)
# reupdate where the center of the cluster is
centers <- apply(x, 2, tapply, clusters, mean)
# save each clust and center for next step
clusterHistory[[i]] <- clusters
centerHistory[[i]] <- centers
}
# return list of clusters and centers
list(clusters=clusterHistory, centers=centerHistory)
}
View(DTM)
nrow(DTM)
train <- DTM[1:70,]
test <- DTM[71:94,]
out_train <- K_means(train, n = 5, n_iter = 10)
out_train <- K_means(train, k = 5, n_iter = 10)
View(out_train)
out_train <- K_means(train, k = 2, n_iter = 10)
